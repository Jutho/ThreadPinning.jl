var documenterSearchIndex = {"docs":
[{"location":"examples/ex_affinity/#exaffinitymask","page":"Respect Process Affinity Mask","title":"Process Affinity Mask","text":"","category":"section"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"In scenarios where the Julia process has a specific affinity mask, e.g. when running under taskset, numactl, or (perhaps) SLURM, you may want to pin your Julia threads in accordance with this affinity mask. To that end, we provide pinthreads(:affinitymask), which pins Julia threads to non-masked CPU-threads (in order, hyperthreads are only used if necessary).","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"For the demonstration below, we consider the following Julia script:","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ cat check.jl \nusing ThreadPinning\nif length(ARGS) > 0 && ARGS[1] == \"pin\"\n    pinthreads(:affinitymask)\nend\nprintln(getcpuids())\nprintln(\"no double occupancies: \", length(unique(getcpuids())) == length(getcpuids()))\nprintln(\"in order: \", issorted(getcpuids()))","category":"page"},{"location":"examples/ex_affinity/#taskset","page":"Respect Process Affinity Mask","title":"taskset","text":"","category":"section"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"Let's use taskset --cpu-list to set the affinity of the Julia process.","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ taskset --cpu-list 0-24 julia --project -t 25 check.jl\n[13, 13, 4, 5, 6, 7, 8, 11, 15, 14, 12, 16, 18, 19, 0, 10, 3, 9, 24, 2, 17, 20, 1, 21, 21]\nno double occupancies: false\nin order: false","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"Note that","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"some Julia threads may run on the same CPU-thread(!) (which is almost certainly not desired), and\nthe order of the Julia thread to CPU-thread mapping is arbitrary (and non-deterministic).","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"We can remedy both points with pinthreads(:affinitymask):","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ taskset --cpu-list 0-24 julia --project -t 25 check.jl pin\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\nno double occupancies: true\nin order: true","category":"page"},{"location":"examples/ex_affinity/#numactl","page":"Respect Process Affinity Mask","title":"numactl","text":"","category":"section"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"The same comments as made for taskset above also apply to numactl --physcpubind. Without pinthreads(:affinitymask):","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ numactl --physcpubind=0-24 julia --project -t 25 check.jl \n[6, 10, 7, 13, 14, 15, 8, 16, 19, 0, 5, 2, 3, 4, 18, 1, 9, 17, 20, 20, 12, 20, 10, 20, 11]\nno double occupancies: false\nin order: false","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"With pinthreads(:affinitymask):","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ numactl --physcpubind=0-24 julia --project -t 25 check.jl pin\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\nno double occupancies: true\nin order: true","category":"page"},{"location":"examples/ex_affinity/#SLURM","page":"Respect Process Affinity Mask","title":"SLURM","text":"","category":"section"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"note: Note\nSLURM settings vary a lot between clusters, in particular affinity related settings. In the following, we visualize the affinity mask set by SLURM at the top of the output files (B means \"this CPU can be used\" whereas - indicates \"this CPU can't be used\" and vertical lines indicate different domains.). Be wary that the same job scripts might not set affinity masks on your cluster!","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ cat slurm_basic.jl \n#!/usr/bin/env sh\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 25\n#SBATCH -o sl_%j.out\n#SBATCH -A pc2-mitarbeiter\n#SBATCH -p all\n#SBATCH -t 00:02:00\n\nsrun -n 1 julia --project -t 25 check.jl ","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"Without pinthreads(:affinitymask):","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ cat sl_2374255.out \ncpu-bind=MASK - cn-0181, task  0  0 [1410285]: mask |BBBBBBBBBBBBBBBBBBBB||||BBBBB---------------|  set\ncpu-bind=MASK - cn-0181, task  0  0 [1410316]: mask |BBBBBBBBBBBBBBBBBBBB||||BBBBB---------------|  set\n[11, 16, 17, 2, 15, 18, 19, 13, 3, 4, 5, 6, 7, 10, 9, 8, 14, 11, 20, 0, 12, 13, 4, 2, 1]\nno double occupancies: false\nin order: false","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"Note that","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"some Julia threads may run on the same CPU-thread(!) (which is almost certainly not desired), and\nthe order of the Julia thread to CPU-thread mapping is arbitrary (and non-deterministic).","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"We can remedy both points with pinthreads(:affinitymask):","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ cat sl_2374256.out \ncpu-bind=MASK - cn-0197, task  0  0 [1507377]: mask |BBBBBBBBBBBBBBBBBBBB||||BBBBB---------------|  set\ncpu-bind=MASK - cn-0197, task  0  0 [1507410]: mask |BBBBBBBBBBBBBBBBBBBB||||BBBBB---------------|  set\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\nno double occupancies: true\nin order: true","category":"page"},{"location":"examples/ex_affinity/#Supplement:-tauto-(Julia-1.9)","page":"Respect Process Affinity Mask","title":"Supplement: -tauto (Julia >= 1.9)","text":"","category":"section"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"For Julia >= 1.9 you can use -tauto to automatically set the number of Julia threads such that it matches the external affinity mask (relevant PR). This is particularly useful when using SLURM, but, for simplicity, we can also showcase it with taskset:","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ taskset --cpu-list 0-24 julia +1.9 -tauto --project check.jl pin\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\nno double occupancies: true\nin order: true","category":"page"},{"location":"examples/ex_affinity/","page":"Respect Process Affinity Mask","title":"Respect Process Affinity Mask","text":"$ taskset --cpu-list 0-11 julia +1.9 -tauto --project check.jl pin\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\nno double occupancies: true\nin order: true","category":"page"},{"location":"refs/api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"refs/api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Pages   = [\"api.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/api/#References-Pinning","page":"API","title":"References - Pinning","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Modules = [ThreadPinning]\nPages   = [\"pinning.jl\", \"pinning_mpi.jl\", \"setaffinity.jl\"]","category":"page"},{"location":"refs/api/#ThreadPinning.pinthread-Tuple{Integer, Integer}","page":"API","title":"ThreadPinning.pinthread","text":"pinthread(threadid::Integer, cpuid::Integer; kwargs...)\n\n\nPin a Julia thread to a specific CPU-thread.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.pinthread-Tuple{Integer}","page":"API","title":"ThreadPinning.pinthread","text":"pinthread(cpuid::Integer; warn) -> Bool\n\n\nPin the calling Julia thread to the given CPU-thread.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.pinthreads","page":"API","title":"ThreadPinning.pinthreads","text":"pinthreads(cpuids[; nthreads, force=true, warn=first_pin_attempt(), threadpool=:default])\n\nPin the first min(length(cpuids), nthreads) Julia threads to an explicit or implicit list of CPU IDs. The latter can be specified in three ways:\n\nexplicitly (e.g. 0:3 or [0,12,4]),\nby passing one of several predefined symbols (e.g. :cores or :sockets),\nby providing a logical specification via helper functions (e.g. node and socket).\n\nSee below for more information.\n\nIf force=false the pinthreads call will only pin threads if this is the first attempt to pin threads with ThreadPinning.jl. Otherwise it will be a no-op. This may be particularly useful for packages that merely want to specify a \"default pinning\".\n\nThe option warn toggles general warnings, such as unwanted interference with BLAS thread settings.\n\nThe keyword argument threadpool can be used to indicate the pool of threads to be pinned. Supported values are :default, :interactive, or :all. (Requires Julia >= 1.9.)\n\n1) Explicit\n\nSimply provide an AbstractVector{<:Integer} of CPU IDs. The latter are expected to be the \"physical\" ids, i.e. as provided by lscpu, and thus start at zero!\n\n2) Predefined Symbols\n\n:cputhreads or :compact: successively pin to all available CPU-threads.\n:cores: spread threads across all available cores, only use hyperthreads if necessary.\n:sockets: spread threads across sockets (round-robin), only use hyperthreads if             necessary. Set compact=true to get compact pinning within each socket.\n:numa: spread threads across NUMA/memory domains (round-robin), only use hyperthreads          if necessary. Set compact=true to get compact pinning within each NUMA/memory          domain.\n:random: pin threads randomly to CPU-threads\n:current: pin threads to the CPU-threads they are currently running on\n:firstn: pin threads to CPU-threads in \"physical\" order (as specified by lscpu).\n:affinitymask: pin threads to different CPU-threads in accordance with their                  affinity mask (must be the same for all of them). By default,                  hyperthreads_last=true.\n\n3) Logical Specification\n\nThe functions node, socket, numa, and core can be used to to specify CPU IDs of/within a certain domain. Moreover, the functions sockets and numas can be used to express a round-robin scatter policy between sockets or NUMA domains, respectively.\n\nExamples (domains):\n\npinthreads(socket(1, 1:3)) # pin to the first 3 cores in the first socket\npinthreads(socket(1, 1:3; compact=true)) # pin to the first 3 CPU-threads in the first socket\npinthreads(numa(2, [2,4,6])) # pin to the second, the fourth, and the sixth cores in the second NUMA/memory domain\npinthreads(node(ncores():-1:1)) # pin threads to cores in reversing order (starting at the end of the node)\npinthreads(sockets()) # scatter threads between sockets, cores before hyperthreads\n\nDifferent domains can be concatenated by providing them in a vector or as separate arguments to pinthreads.\n\nExamples (concatenation):\n\npinthreads([socket(1, 1:3), numa(2, 4:6)])\npinthreads(socket(1, 1:3), numa(2, 4:6))\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.unpinthread-Tuple{Integer}","page":"API","title":"ThreadPinning.unpinthread","text":"unpinthread(threadid)\n\n\nUnpins the given Julia thread by setting the affinity mask to all unity. Afterwards, the OS is free to move the Julia thread from one CPU thread to another.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.unpinthreads-Tuple{}","page":"API","title":"ThreadPinning.unpinthreads","text":"Unpins all Julia threads by setting the affinity mask of all threads to all unity. Afterwards, the OS is free to move any Julia thread from one CPU thread to another.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.pinthreads_mpi-Tuple{Symbol, Vararg{Any}}","page":"API","title":"ThreadPinning.pinthreads_mpi","text":"pinthreads_mpi(symbol, rank, nranks; nthreads_per_rank, compact, kwargs...)\n\nPin MPI ranks, that is, their respective Julia thread(s), to (subsets of) domains (e.g. sockets or memory domains). Specifically, when calling this function on all MPI ranks, the latter will be distributed in a round-robin fashion among the specified domains such that their Julia threads are pinned to non-overlapping ranges of CPU-threads within the domain.\n\nValid options for symbol are :sockets and :numa.\n\nIf compact=false (default), physical cores are occupied before hyperthreads. Otherwise, CPU-cores - with potentially multiple CPU-threads - are filled up one after another (compact pinning).\n\nThe keyword argument nthreads_per_rank (default Threads.nthreads()) can be used to pin only a subset of the available Julia threads per MPI rank.\n\nNote: As per usual for MPI, rank starts at zero.\n\nExample:\n\nusing ThreadPinning\nusing MPI\ncomm = MPI.COMM_WORLD\nnranks = MPI.Comm_size(comm)\nrank = MPI.Comm_rank(comm)\npinthreads_mpi(:sockets, rank, nranks)\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.setaffinity-Tuple{AbstractVector{<:Integer}}","page":"API","title":"ThreadPinning.setaffinity","text":"setaffinity(cpuids::AbstractVector{<:Integer})\n\n\nSet the affinity of the calling Julia thread to the given CPU-threads.\n\nExample:\n\nsetaffinity(socket(1)) # set the affinity to the first socket\nsetaffinity(numa(2)) # set the affinity to the second NUMA domain\nsetaffinity(socket(1, 1:3)) # set the affinity to the first three cores in the first NUMA domain\nsetaffinity([1,3,5]) # set the affinity to the CPU-threads with the IDs 1, 3, and 5.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.setaffinity-Tuple{Integer, AbstractVector{<:Integer}}","page":"API","title":"ThreadPinning.setaffinity","text":"setaffinity(\n    threadid::Integer,\n    cpuids::AbstractVector{<:Integer};\n    kwargs...\n)\n\n\nSet the affinity of a specific Julia thread to the given CPU-threads.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#References-Querying","page":"API","title":"References - Querying","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Modules = [ThreadPinning]\nPages   = [\"querying.jl\", \"threadinfo.jl\"]","category":"page"},{"location":"refs/api/#ThreadPinning.affinitymask2cpuids-Tuple{Any}","page":"API","title":"ThreadPinning.affinitymask2cpuids","text":"Get the CPU-thread IDs associated with the given affinity mask.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.core","page":"API","title":"ThreadPinning.core","text":"core(i)\ncore(i, idcs; shuffle, kwargs...)\n\n\nRepresents the CPU ID domain of core i (logical index, starts at 1). Uses compact ordering by default. Set shuffle=true to randomize.\n\nOptional second argument: Logical indices to select a subset of the domain.\n\nTo be used as input argument for pinthreads. What it actually returns is an implementation detail!\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.cpuids_all-Tuple{}","page":"API","title":"ThreadPinning.cpuids_all","text":"Returns a Vector{Int} which lists all valid CPUIDs. There is no guarantee about the order except that it is the same as in lscpu.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.cpuids_per_core-Tuple{}","page":"API","title":"ThreadPinning.cpuids_per_core","text":"Returns a Vector{Vector{Int}} which indicates the CPUIDs associated with the available physical cores\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.cpuids_per_node-Tuple{}","page":"API","title":"ThreadPinning.cpuids_per_node","text":"cpuids_per_node(; compact)\n\n\nReturns a Vector{Int} which indicates the CPUIDs associated with the available node. Physical cores come first. Set compact=true to get compact ordering.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.cpuids_per_numa-Tuple{}","page":"API","title":"ThreadPinning.cpuids_per_numa","text":"cpuids_per_numa(; compact)\n\n\nReturns a Vector{Vector{Int}} which indicates the CPUIDs associated with the available NUMA domains. Within each memory domain, physical cores come first. Set compact=true to get compact ordering instead.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.cpuids_per_socket-Tuple{}","page":"API","title":"ThreadPinning.cpuids_per_socket","text":"cpuids_per_socket(; compact)\n\n\nReturns a Vector{Vector{Int}} which indicates the CPUIDs associated with the available CPU sockets. Within each socket, physical cores come first. Set compact=true to get compact ordering instead.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.get_affinity_mask","page":"API","title":"ThreadPinning.get_affinity_mask","text":"get_affinity_mask()\nget_affinity_mask(tid)\n\n\nGet the affinity mask of the given Julia Thread\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.get_cpuids_from_affinity_mask","page":"API","title":"ThreadPinning.get_cpuids_from_affinity_mask","text":"get_cpuids_from_affinity_mask()\nget_cpuids_from_affinity_mask(tid)\n\n\nGet the IDs of the CPU-threads associated with the affinity mask of the given Julia Thread\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.getcpuid-Tuple{Integer}","page":"API","title":"ThreadPinning.getcpuid","text":"getcpuid(threadid)\n\n\nReturns the ID of the CPU thread on which the given Julia thread (threadid) is currently running.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.getcpuid-Tuple{}","page":"API","title":"ThreadPinning.getcpuid","text":"Returns the ID of the CPU thread on which the calling thread is currently running.\n\nSee sched_getcpu for more information.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.getcpuids-Tuple{}","page":"API","title":"ThreadPinning.getcpuids","text":"Returns the IDs of the CPU-threads on which the Julia threads are currently running.\n\nSee getcpuid for more information.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.getnumanode-Tuple{Integer}","page":"API","title":"ThreadPinning.getnumanode","text":"getnumanode(threadid)\n\n\nReturns the ID (starting at zero) of the NUMA node corresponding to the CPU thread on which the given Julia thread (threadid) is currently running.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.getnumanode-Tuple{}","page":"API","title":"ThreadPinning.getnumanode","text":"Returns the ID (starting at zero) of the NUMA node corresponding to the CPU thread on which the calling thread is currently running.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.getnumanodes-Tuple{}","page":"API","title":"ThreadPinning.getnumanodes","text":"Returns the ID (starting at zero) of the NUMA nodes corresponding to the CPU threads on which the Julia threads are currently running.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.hyperthreading_is_enabled-Tuple{}","page":"API","title":"ThreadPinning.hyperthreading_is_enabled","text":"Check whether hyperthreading is enabled.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ishyperthread-Tuple{Integer}","page":"API","title":"ThreadPinning.ishyperthread","text":"ishyperthread(cpuid)\n\n\nCheck whether the given CPU-thread is a hyperthread (i.e. the second CPU-thread associated with a CPU-core).\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncores-Tuple{}","page":"API","title":"ThreadPinning.ncores","text":"Number of cores (i.e. excluding hyperthreads)\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncores_per_numa-Tuple{}","page":"API","title":"ThreadPinning.ncores_per_numa","text":"Number of CPU-cores per NUMA domain\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncores_per_socket-Tuple{}","page":"API","title":"ThreadPinning.ncores_per_socket","text":"Number of CPU-cores per socket\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncputhreads-Tuple{}","page":"API","title":"ThreadPinning.ncputhreads","text":"Number of CPU-threads\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncputhreads_per_core-Tuple{}","page":"API","title":"ThreadPinning.ncputhreads_per_core","text":"Number of CPU-threads per core\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncputhreads_per_numa-Tuple{}","page":"API","title":"ThreadPinning.ncputhreads_per_numa","text":"Number of CPU-threads per NUMA domain\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.ncputhreads_per_socket-Tuple{}","page":"API","title":"ThreadPinning.ncputhreads_per_socket","text":"Number of CPU-threads per socket\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.nnuma-Tuple{}","page":"API","title":"ThreadPinning.nnuma","text":"Number of NUMA nodes\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.node","page":"API","title":"ThreadPinning.node","text":"node()\nnode(idcs; shuffle, kwargs...)\n\n\nRepresents the CPU ID domain of the entire node/system. By default, cores will be used first and hyperthreads will only be used if necessary. Provide compact=true to get compact ordering instead. Set shuffle=true to randomize. Set shuffle=true to randomize.\n\nOptional first argument: Logical indices to select a subset of the domain.\n\nTo be used as input argument for pinthreads. What it actually returns is an implementation detail!\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.nsockets-Tuple{}","page":"API","title":"ThreadPinning.nsockets","text":"Number of CPU sockets\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.numa","page":"API","title":"ThreadPinning.numa","text":"numa(i)\nnuma(i, idcs; shuffle, kwargs...)\n\n\nRepresents the CPU ID domain of NUMA/memory domain i (logical index, starts at 1). By default, cores will be used first and hyperthreads will only be used if necessary. Provide compact=true to get compact ordering instead. Set shuffle=true to randomize.\n\nOptional second argument: Logical indices to select a subset of the domain.\n\nTo be used as input argument for pinthreads. What it actually returns is an implementation detail!\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.numas","page":"API","title":"ThreadPinning.numas","text":"numas()\nnumas(idcs; shuffle, kwargs...)\n\n\nRepresents the CPU IDs of the system as obtained by a round-robin scattering between NUMA/memory domain. By default, within each memory domain, cores will be used first and hyperthreads will only be used if necessary. Provide compact=true to get compact ordering within each memory domain. Set shuffle=true to randomize.\n\nOptional first argument: Logical indices to select a subset of the domain.\n\nTo be used as input argument for pinthreads. What it actually returns is an implementation detail!\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.print_affinity_mask","page":"API","title":"ThreadPinning.print_affinity_mask","text":"print_affinity_mask()\nprint_affinity_mask(tid; io, kwargs...)\n\n\nPrint the affinity mask of a Julia thread.\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.print_affinity_masks-Tuple{}","page":"API","title":"ThreadPinning.print_affinity_masks","text":"print_affinity_masks(; threadpool, io, kwargs...)\n\n\nPrint the affinity masks of all Julia threads.\n\n\n\n\n\n","category":"method"},{"location":"refs/api/#ThreadPinning.socket","page":"API","title":"ThreadPinning.socket","text":"socket(i)\nsocket(i, idcs; shuffle, kwargs...)\n\n\nRepresents the CPU ID domain of socket i (logical index, starts at 1). By default, cores will be used first and hyperthreads will only be used if necessary. Provide compact=true to get compact ordering instead. Set shuffle=true to randomize.\n\nOptional second argument: Logical indices to select a subset of the domain.\n\nTo be used as input argument for pinthreads. What it actually returns is an implementation detail!\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.sockets","page":"API","title":"ThreadPinning.sockets","text":"sockets()\nsockets(idcs; shuffle, kwargs...)\n\n\nRepresents the CPU IDs of the system as obtained by a round-robin scattering between sockets. By default, within each socket, cores will be used first and hyperthreads will only be used if necessary. Provide compact=true to get compact ordering within each socket. Set shuffle=true to randomize.\n\nOptional first argument: Logical indices to select a subset of the domain.\n\nTo be used as input argument for pinthreads. What it actually returns is an implementation detail!\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#ThreadPinning.threadinfo","page":"API","title":"ThreadPinning.threadinfo","text":"threadinfo()\nthreadinfo(\n    io;\n    blas,\n    hints,\n    color,\n    masks,\n    groupby,\n    threadpool,\n    slurm,\n    kwargs...\n)\n\n\nPrint information about Julia threads, e.g. on which CPU-threads (i.e. cores if hyperthreading is disabled) they are running.\n\nKeyword arguments:\n\ncolor (default: true): Toggle between colored and black-and-white output.\nblocksize (default: 32): Wrap to a new line after blocksize many CPU-threads.\nhyperthreading (default: true): If true, we (try to) highlight CPU-threads associated with hyperthreading in the color=true output.\nblas (default: false): Show information about BLAS threads as well.\nslurm (default: false): Only show the part of the system that is covered by the active SLURM session.\nhints (default: false): Give some hints about how to improve the threading related settings.\ngroupby (default: :sockets): Options are :sockets, :numa, :cores, or :none.\nmasks (default: false): Show the affinity masks of all Julia threads.\nthreadpool (default: :default): Only consider Julia threads in the given thread pool.                                 Supported values are :default, :interactive, and                                 :all. Only works for Julia >= 1.9.\n\n\n\n\n\n","category":"function"},{"location":"refs/prefs/#Preferences","page":"Preferences","title":"Preferences","text":"","category":"section"},{"location":"refs/prefs/#Index","page":"Preferences","title":"Index","text":"","category":"section"},{"location":"refs/prefs/","page":"Preferences","title":"Preferences","text":"Pages   = [\"prefs.md\"]\nOrder   = [:function]","category":"page"},{"location":"refs/prefs/#References","page":"Preferences","title":"References","text":"","category":"section"},{"location":"refs/prefs/","page":"Preferences","title":"Preferences","text":"Modules = [ThreadPinning.Prefs]\nPages   = [\"preferences.jl\"]","category":"page"},{"location":"refs/prefs/#ThreadPinning.Prefs.clear-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.clear","text":"Clear all ThreadPinning.jl related preferences\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.get_autoupdate-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.get_autoupdate","text":"Get the autoupdate preference. Returns nothing if not set.\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.get_likwidpin-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.get_likwidpin","text":"Get the likwidpin preference. Returns nothing if not set.\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.get_os_warning-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.get_os_warning","text":"Get the OS warning preference. Returns nothing if not set.\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.get_pin-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.get_pin","text":"Get the pin preference. Returns nothing if not set.\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.has_autoupdate-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.has_autoupdate","text":"Query whether the autoupdate preference is set\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.has_likwidpin-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.has_likwidpin","text":"Query whether the likwidpin preference is set\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.has_os_warning-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.has_os_warning","text":"Query whether the OS warning preference is set\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.has_pin-Tuple{}","page":"Preferences","title":"ThreadPinning.Prefs.has_pin","text":"Query whether the pin preference is set\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.set_autoupdate-Tuple{Bool}","page":"Preferences","title":"ThreadPinning.Prefs.set_autoupdate","text":"set_autoupdate(b)\n\n\nSet the autoupdate preference\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.set_likwidpin-Tuple{AbstractString}","page":"Preferences","title":"ThreadPinning.Prefs.set_likwidpin","text":"set_likwidpin(s)\n\n\nSet the likwidpin preference\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.set_os_warning-Tuple{Bool}","page":"Preferences","title":"ThreadPinning.Prefs.set_os_warning","text":"set_os_warning(b)\n\n\nSet the OS warning preference\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.set_pin-Tuple{Union{AbstractString, Symbol}}","page":"Preferences","title":"ThreadPinning.Prefs.set_pin","text":"set_pin(s)\n\n\nSet the pin preference\n\n\n\n\n\n","category":"method"},{"location":"refs/prefs/#ThreadPinning.Prefs.showall","page":"Preferences","title":"ThreadPinning.Prefs.showall","text":"Show all ThreadPinning.jl related preferences\n\n\n\n\n\n","category":"function"},{"location":"examples/ex_pinning_julia_threads/#Pinning-Julia-Threads","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"The most important functions are pinthreads and threadinfo. The former allows you to pin threads. The latter visualizes the current thread-processor mapping and the system topology. Please check out the comprehensive documentation of these functions for detailed information. ","category":"page"},{"location":"examples/ex_pinning_julia_threads/#Typical-usage","page":"Pinning Julia Threads","title":"Typical usage","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/#pinthreads","page":"Pinning Julia Threads","title":"pinthreads","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Pinning your threads is as simple as putting the following at the top of your Julia code:","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"using ThreadPinning\npinthreads(:cores)","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"This will successively pin all Julia threads to CPU-cores in logical order, avoiding hyperthreads if possible. Of course, you can replace :cores by all the options supported by pinthreads. Conceptually, there are three different formats to specify your desired thread-processor mapping:","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"explicit lists of CPU IDs (e.g. 0:3 or [0,12,4]),\npredefined symbols (e.g. :cores or :sockets),\nlogical specification of domains via helper functions (e.g. node and socket).","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"For example, instead of pinthreads(:cores) above, you could write pinthreads(1:2:10) or pinthreads(socket(1,1:3), numa(2,2:5)). Again, see pinthreads for more information.","category":"page"},{"location":"examples/ex_pinning_julia_threads/#threadinfo","page":"Pinning Julia Threads","title":"threadinfo","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"To check and visualize the current pinning you can use threadinfo to get something like this.","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"(Image: threadinfo_ht_long.png)","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"As you can see, this image is taken on a dual-socket system (indicated by the two | .... | sections) where each CPU has 20 CPU-cores and Julia has been started with 40 threads. Hyperthreading is enabled - the greyed out numbers indicate hyperthreads/SMT-threads - with two CPU-threads per core.","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Note that threadinfo has a few keyword arguments that let you change or tune the output. The most important ones are probably groupby and color. The former allows you to switch from socket to, say, NUMA/memory domain visualization (groupby=:numa). The latter allows you to switch to non-colored output (see below).","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"julia> using ThreadPinning\n\njulia> threadinfo(; color=false)\n\n| 0,1,_,3,4,_,_,7,8,_,10,_,_,_,_,_,\n  16,17,_,_,40,_,42,_,_,_,_,47,48,49,50,51,\n  _,_,54,_,_,57,58,_ |\n| _,21,22,23,_,_,_,_,28,29,30,_,32,33,_,35,\n  _,_,38,39,60,61,62,63,64,65,_,_,68,_,_,_,\n  72,73,74,_,_,_,_,_ |\n\n# = Julia thread, # = HT, # = Julia thread on HT, | = Socket seperator\n\nJulia threads: 40\n├ Occupied CPU-threads: 40\n└ Mapping (Thread => CPUID): 1 => 63, 2 => 64, 3 => 17, 4 => 68, 5 => 4, ...\n\n\njulia> pinthreads(:cores)\n\njulia> threadinfo(; color=false)\n\n| 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,\n  16,17,18,19,_,_,_,_,_,_,_,_,_,_,_,_,\n  _,_,_,_,_,_,_,_ |\n| 20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,\n  36,37,38,39,_,_,_,_,_,_,_,_,_,_,_,_,\n  _,_,_,_,_,_,_,_ |\n\n# = Julia thread, # = HT, # = Julia thread on HT, | = Socket seperator\n\nJulia threads: 40\n├ Occupied CPU-threads: 40\n└ Mapping (Thread => CPUID): 1 => 0, 2 => 1, 3 => 2, 4 => 3, 5 => 4, ...\n\n\njulia> pinthreads(:cputhreads)\n\njulia> threadinfo(; color=false)\n\n| 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,\n  16,17,18,19,40,41,42,43,44,45,46,47,48,49,50,51,\n  52,53,54,55,56,57,58,59 |\n| _,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,\n  _,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,\n  _,_,_,_,_,_,_,_ |\n\n# = Julia thread, # = HT, # = Julia thread on HT, | = Socket seperator\n\nJulia threads: 40\n├ Occupied CPU-threads: 40\n└ Mapping (Thread => CPUID): 1 => 0, 2 => 40, 3 => 1, 4 => 41, 5 => 2, ...","category":"page"},{"location":"examples/ex_pinning_julia_threads/#envvars","page":"Pinning Julia Threads","title":"Pinning via environment variables","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Sometimes it can be useful to specify a desired thread-processor mapping before starting Julia, that is, without using pinthreads explicitly in your Julia code. A Julia-built-in example for this is JULIA_EXCLUSIVE=1. Similarly, ThreadPinning.jl offers pinning threads via environment variables. In this case, Julia Threads will get pinned during the initialization of the ThreadPinning package, i.e. when running using ThreadPinning. Note, though, that explicit pinthreads statements take precedence over these environment variables.","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Concretely, the supported environment variables are","category":"page"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"JULIA_PIN: Can be set to any symbol supported by pinthreads (e.g. JULIA_PIN=cores). Capitalization doesn't matter.\nJULIA_LIKWID_PIN: Can be set to any string supported by pinthreads_likwidpin (e.g. JULIA_LIKWID_PIN=S0:1-3@S1:4,5,6). Capitalization does matter.","category":"page"},{"location":"examples/ex_pinning_julia_threads/#prefs","page":"Pinning Julia Threads","title":"Pinning via preferences","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"To specify a certain pinning pattern more permanently, e.g., on a per-project basis, you can use preferences. ThreadPinning.jl provides the relevant functionality in the ThreadPinning.Prefs module. Note that environment variables and explicit pinthreads statements take precedence over these preferences.","category":"page"},{"location":"examples/ex_pinning_julia_threads/#Default-pinning-(for-packages)","page":"Pinning Julia Threads","title":"Default pinning (for packages)","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"If you're developing a package you may want to provide a reasonable default pinning. If you would naively use pinthreads for this, you would enforce a certain pinning irrespective of what the user might have specified manually. This is because pinthreads has the highest precedence. To lower the latter you can set force=false in your pinthreads call, e.g. pinthreads(:cores; force=false). This way, a user can overwrite your default pinning (:cores in this example) by using environment variables, preferences, or calling pinthreads manually before running your package code.","category":"page"},{"location":"examples/ex_pinning_julia_threads/#Unpinning","page":"Pinning Julia Threads","title":"Unpinning","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"We provide functions unpinthread(threadid) and unpinthreads() to unpin a specific or all Julia threads, respectively. This is realized by setting the thread affinity mask to all ones. While technically not really unpinning threads, you might also want to consider using pinthreads(:random) for \"fake unpinning\" in benchmarks as it does randomize the thread placing but keeps it fixed to reduce measurement fluctuations.","category":"page"},{"location":"examples/ex_pinning_julia_threads/#likwid-pin-compatible-input","page":"Pinning Julia Threads","title":"likwid-pin-compatible input","text":"","category":"section"},{"location":"examples/ex_pinning_julia_threads/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Separate from pinthreads, used and described above, we offer pinthreads_likwidpin which, ideally, should handle all inputs that are supported by the -c option of likwid-pin (e.g. S0:1-3@S1:2,4,5 or E:N:4:2:4). If you encounter an input that doesn't work as expected, please file an issue.","category":"page"},{"location":"refs/blaslapack/#BLAS/LAPACK","page":"BLAS/LAPACK","title":"BLAS/LAPACK","text":"","category":"section"},{"location":"refs/blaslapack/","page":"BLAS/LAPACK","title":"BLAS/LAPACK","text":"warning: Warning\nThis section isn't part of the official API. Things might change at any point without further notice.","category":"page"},{"location":"refs/blaslapack/#Index","page":"BLAS/LAPACK","title":"Index","text":"","category":"section"},{"location":"refs/blaslapack/","page":"BLAS/LAPACK","title":"BLAS/LAPACK","text":"Pages   = [\"blaslapack.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/blaslapack/#References-OpenBLAS","page":"BLAS/LAPACK","title":"References - OpenBLAS","text":"","category":"section"},{"location":"refs/blaslapack/","page":"BLAS/LAPACK","title":"BLAS/LAPACK","text":"Modules = [ThreadPinning]\nPages   = [\"openblas.jl\"]","category":"page"},{"location":"refs/blaslapack/#ThreadPinning.openblas_getcpuid","page":"BLAS/LAPACK","title":"ThreadPinning.openblas_getcpuid","text":"Returns the ID of the CPU thread on which the i-th OpenBLAS thread is currently running.\n\nnote: Note\nAvailable as of Julia 1.9.\n\n\n\n\n\n","category":"function"},{"location":"refs/blaslapack/#ThreadPinning.openblas_getcpuids","page":"BLAS/LAPACK","title":"ThreadPinning.openblas_getcpuids","text":"Returns the IDs of the CPU-threads on which the OpenBLAS threads are currently running.\n\nnote: Note\nAvailable as of Julia 1.9.\n\n\n\n\n\n","category":"function"},{"location":"refs/blaslapack/#ThreadPinning.openblas_nthreads-Tuple{}","page":"BLAS/LAPACK","title":"ThreadPinning.openblas_nthreads","text":"Query the number of OpenBLAS threads.\n\n\n\n\n\n","category":"method"},{"location":"refs/blaslapack/#ThreadPinning.openblas_pinthreads-Tuple{AbstractVector{<:Integer}}","page":"BLAS/LAPACK","title":"ThreadPinning.openblas_pinthreads","text":"openblas_pinthreads(\n    cpuids;\n    nthreads,\n    juliathread,\n    kwargs...\n)\n\n\nPin the available OpenBLAS threads to the given CPU IDs. Currently, only explict pinning is possible.\n\n\n\n\n\n","category":"method"},{"location":"refs/blaslapack/#ThreadPinning.openblas_print_affinity_masks","page":"BLAS/LAPACK","title":"ThreadPinning.openblas_print_affinity_masks","text":"Print the affinity masks of all OpenBLAS threads.\n\nnote: Note\nAvailable as of Julia 1.9.\n\n\n\n\n\n","category":"function"},{"location":"refs/blaslapack/#References-Intel-MKL","page":"BLAS/LAPACK","title":"References - Intel MKL","text":"","category":"section"},{"location":"refs/blaslapack/","page":"BLAS/LAPACK","title":"BLAS/LAPACK","text":"Modules = [ThreadPinning]\nPages   = [\"mkl.jl\"]","category":"page"},{"location":"refs/blaslapack/#ThreadPinning.mkl_fullpath-Tuple{}","page":"BLAS/LAPACK","title":"ThreadPinning.mkl_fullpath","text":"mkl_fullpath(; force_update)\n\n\nReturns the full path to the libmkl_rt library if the latter is loaded. Will try to locate the library and, if successfull, will cache the result. Throws an error otherwise.\n\nTo force an update of the cache, provide force_update=true.\n\n\n\n\n\n","category":"method"},{"location":"refs/blaslapack/#ThreadPinning.mkl_get_dynamic-Tuple{}","page":"BLAS/LAPACK","title":"ThreadPinning.mkl_get_dynamic","text":"mkl_get_dynamic()\n\nWrapper around the MKL function mkl_get_dynamic.\n\n\n\n\n\n","category":"method"},{"location":"refs/blaslapack/#ThreadPinning.mkl_is_loaded-Tuple{}","page":"BLAS/LAPACK","title":"ThreadPinning.mkl_is_loaded","text":"Check whether Intel MKL is currently loaded via libblastrampoline (Julia >= 1.7) or is available in Libdl.dllist() (Julia 1.6).\n\n\n\n\n\n","category":"method"},{"location":"refs/blaslapack/#ThreadPinning.mkl_set_dynamic-Tuple{Integer}","page":"BLAS/LAPACK","title":"ThreadPinning.mkl_set_dynamic","text":"mkl_set_dynamic(flag)\n\n\nWrapper around the MKL function mkl_set_dynamic.\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#LibX","page":"LibX","title":"LibX","text":"","category":"section"},{"location":"refs/libX/","page":"LibX","title":"LibX","text":"warning: Warning\nThis section isn't part of the official API. Things might change at any point without further notice.","category":"page"},{"location":"refs/libX/","page":"LibX","title":"LibX","text":"Wrappers around some functionality provided by libc, libpthread, and libuv.","category":"page"},{"location":"refs/libX/#Index","page":"LibX","title":"Index","text":"","category":"section"},{"location":"refs/libX/","page":"LibX","title":"LibX","text":"Pages   = [\"libX.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/libX/#References","page":"LibX","title":"References","text":"","category":"section"},{"location":"refs/libX/","page":"LibX","title":"LibX","text":"Modules = [ThreadPinning]\nPages   = [\"libuv.jl\", \"libc.jl\", \"libpthread.jl\"]","category":"page"},{"location":"refs/libX/#ThreadPinning.uv_cpumask_size-Tuple{}","page":"LibX","title":"ThreadPinning.uv_cpumask_size","text":"Returns the maximum size of the mask used for process/thread affinities, or UV_ENOTSUP if affinities are not supported on the current platform.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.uv_thread_getaffinity-Tuple{Any, Any, Any}","page":"LibX","title":"ThreadPinning.uv_thread_getaffinity","text":"uv_thread_getaffinity(self_ref, cpumask, masksize)\n\n\nGets the specified thread's affinity setting. On Unix, this maps the cpu_set_t returned by pthread_getaffinity_np(3) to bytes in cpumask.\n\nThe masksize specifies the number of entries (bytes) in cpumask, and must be greater-than-or-equal-to uv_cpumask_size.\n\nNote: Thread affinity getting is not atomic on Windows and unsupported on macOS.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.uv_thread_getaffinity-Tuple{}","page":"LibX","title":"ThreadPinning.uv_thread_getaffinity","text":"uv_thread_getaffinity()\n\n\nQuery the calling thread's affinity.\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.uv_thread_self-Tuple{}","page":"LibX","title":"ThreadPinning.uv_thread_self","text":"Ref: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.uv_thread_setaffinity-NTuple{4, Any}","page":"LibX","title":"ThreadPinning.uv_thread_setaffinity","text":"uv_thread_setaffinity(self_ref, cpumask, oldmask, masksize)\n\n\nSets the specified thread's affinity to cpumask, which is specified in bytes. Optionally returning the previous affinity setting in oldmask. On Unix, uses pthread_getaffinity_np(3) to get the affinity setting and maps the cpu_set_t to bytes in oldmask. Then maps the bytes in cpumask to a cpu_set_t and uses pthread_setaffinity_np(3). On Windows, maps the bytes in cpumask to a bitmask and uses SetThreadAffinityMask() which returns the previous affinity setting.\n\nThe masksize specifies the number of entries (bytes) in cpumask / oldmask, and must be greater-than-or-equal-to uv_cpumask_size().\n\nNote: Thread affinity setting is not atomic on Windows and unsupported on macOS.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.uv_thread_setaffinity-Tuple{Integer}","page":"LibX","title":"ThreadPinning.uv_thread_setaffinity","text":"uv_thread_setaffinity(procid)\n\n\nSet the calling thread's affinity to procid.\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.getpid-Tuple{}","page":"LibX","title":"ThreadPinning.getpid","text":"Returns the process ID (PID) of the calling process.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.getppid-Tuple{}","page":"LibX","title":"ThreadPinning.getppid","text":"Returns the process ID of the parent of the calling process.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.gettid-Tuple{}","page":"LibX","title":"ThreadPinning.gettid","text":"Returns the caller's thread ID (TID).  In a single- threaded process, the thread ID is equal to the process ID (PID, as returned by getpid(2)).  In a multithreaded process, all threads have the same PID, but each one has a unique TID.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.sched_getcpu-Tuple{}","page":"LibX","title":"ThreadPinning.sched_getcpu","text":"Returns the number of the CPU on which the calling thread is currently executing.\n\nRef: docs\n\n\n\n\n\n","category":"method"},{"location":"refs/libX/#ThreadPinning.pthread_set_affinity_mask-Tuple{Any}","page":"LibX","title":"ThreadPinning.pthread_set_affinity_mask","text":"The input mask should be either of the following:\n\na BitArray indicating the mask directly\na vector of cpuids (the mask will be constructed automatically)\n\n\n\n\n\n","category":"method"},{"location":"refs/latency/#Latency","page":"Latency","title":"Latency","text":"","category":"section"},{"location":"refs/latency/","page":"Latency","title":"Latency","text":"warning: Warning\nThis section isn't part of the official API. Things might change at any point without further notice.","category":"page"},{"location":"refs/latency/#Index","page":"Latency","title":"Index","text":"","category":"section"},{"location":"refs/latency/","page":"Latency","title":"Latency","text":"Pages   = [\"latency.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/latency/#References","page":"Latency","title":"References","text":"","category":"section"},{"location":"refs/latency/","page":"Latency","title":"Latency","text":"Modules = [ThreadPinning]\nPages   = [\"latency.jl\"]","category":"page"},{"location":"refs/latency/#ThreadPinning.bench_core2core_latency","page":"Latency","title":"ThreadPinning.bench_core2core_latency","text":"bench_core2core_latency([cpuids; nbench = 5, nsamples::Integer = 100, mode::Symbol = :min])\n\nA tool for measuring core-to-core latency (i.e. inter-core latency) in nanoseconds.\n\nThe measured latencies correspond to a full roundtrip between two cores. Divide them by two to obtain an estimate for the time needed to fetch data from another core.\n\nImportant: At least two Julia threads are required (julia -t2)!\n\nRefs: Largely inspired by rigtorp/c2clat and ajakubek/core-latency.\n\n\n\n\n\n","category":"function"},{"location":"explanations/linux/#linux","page":"Why only Linux?","title":"Why is Only Linux Supported?","text":"","category":"section"},{"location":"explanations/linux/","page":"Why only Linux?","title":"Why only Linux?","text":"For ThreadPinning.jl to fully work, the operating system must support querying and setting the affinity of Julia threads (pthreads). This is readily possible on Linux but less so or more complicated on Windows and macOS. See below for more information.","category":"page"},{"location":"explanations/linux/#Linux","page":"Why only Linux?","title":"Linux","text":"","category":"section"},{"location":"explanations/linux/","page":"Why only Linux?","title":"Why only Linux?","text":"We use libcs sched_getcpu to query the ID of the CPU-Thread that is currently running a given Julia thread. For pinning, we use uv_thread_setaffinity provided by libuv. For the corresponding Julia wrappers of these libraries, see LibX.","category":"page"},{"location":"explanations/linux/#Windows","page":"Why only Linux?","title":"Windows","text":"","category":"section"},{"location":"explanations/linux/","page":"Why only Linux?","title":"Why only Linux?","text":"I neither have much knowledge about Windows APIs nor proper access to Windows machines. Nonetheless, I've made an initial attempt to add partial Windows support in this PR. If you're eager to have Windows fully supported, please take matters into your own hand. I'm happy to offer help and review a PR from you.","category":"page"},{"location":"explanations/linux/#macOS","page":"Why only Linux?","title":"macOS","text":"","category":"section"},{"location":"explanations/linux/","page":"Why only Linux?","title":"Why only Linux?","text":"Unfortunately, macOS doesn't support any way to pin threads to specific CPU-threads. It is thus very unlikely that macOS can ever be fully supported.","category":"page"},{"location":"explanations/linux/","page":"Why only Linux?","title":"Why only Linux?","text":"Having said that, there seems to be a (very?) limited Thread Affinity API for which support might be added. This is unlikely to ever be on my agenda though.","category":"page"},{"location":"refs/utility/#Utility","page":"Utility","title":"Utility","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"warning: Warning\nThis section isn't part of the official API. Things might change at any point without further notice.","category":"page"},{"location":"refs/utility/#Index","page":"Utility","title":"Index","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"Pages   = [\"utility.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/utility/#References","page":"Utility","title":"References","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"Modules = [ThreadPinning]\nPages   = [\"utility.jl\"]","category":"page"},{"location":"refs/utility/#ThreadPinning.BLAS_lib-Tuple{}","page":"Utility","title":"ThreadPinning.BLAS_lib","text":"Returns the name of the loaded BLAS library (the first, if multiple are loaded).\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#ThreadPinning.interweave-Tuple{Vararg{AbstractVector}}","page":"Utility","title":"ThreadPinning.interweave","text":"interweave(arrays::AbstractVector...) -> Any\n\n\nExamples\n\ninterweave([1,2,3,4], [5,6,7,8]) == [1,5,2,6,3,7,4,8]\n\ninterweave(1:4, 5:8, 9:12) == [1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12]\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#ThreadPinning.nblasthreads-Tuple{}","page":"Utility","title":"ThreadPinning.nblasthreads","text":"Number of BLAS threads.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#ThreadPinning.@tspawnat-Tuple{Any, Any}","page":"Utility","title":"ThreadPinning.@tspawnat","text":"@tspawnat tid -> task\n\nMimics Threads.@spawn, but assigns the task to thread tid (with sticky = true).\n\nNote for Julia >= 1.9: Threads in the :interactive thread pool come after those in :default. Hence, use a thread id tid > nthreads(:default) to spawn computations on \"interactive\" threads.\n\nExample\n\njulia> t = @tspawnat 4 Threads.threadid()\nTask (runnable) @0x0000000010743c70\njulia> fetch(t)\n4\n\n\n\n\n\n","category":"macro"},{"location":"explanations/why/#why","page":"Why Pin Julia Threads?","title":"Why Pin Julia Threads?","text":"","category":"section"},{"location":"explanations/why/","page":"Why Pin Julia Threads?","title":"Why Pin Julia Threads?","text":"Because","category":"page"},{"location":"explanations/why/","page":"Why Pin Julia Threads?","title":"Why Pin Julia Threads?","text":"it effects performance (MFlops/s), in particular on HPC clusters with multiple NUMA domains\nit allows you to utilize performance counters inside of CPU-cores for hardware-performance monitoring\nit makes performance benchmarks more reliable (i.e. less random/noisy)\n...","category":"page"},{"location":"examples/ex_mpi/#MPI","page":"MPI and MPI + Threads","title":"MPI","text":"","category":"section"},{"location":"examples/ex_mpi/#SLURM","page":"MPI and MPI + Threads","title":"SLURM","text":"","category":"section"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"In this section, we'll focus on MPI applications that run under SLURM (or a similar job scheduler). On most systems, the latter sets the affinity mask of the Julia processes (MPI ranks) based on the options set by the user (e.g. via #SBATCH). Consequently, one has to do little to nothing on the Julia side to achieve the desired pinning pattern.","category":"page"},{"location":"examples/ex_mpi/#MPI-only","page":"MPI and MPI + Threads","title":"MPI only","text":"","category":"section"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"If your MPI-parallel application is single threaded (i.e. one Julia thread per MPI rank), you likely don't have to do anything on the Julia side to pin the MPI ranks. Instead, you can just use the SLURM options.","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"Multinode example, 1 MPI rank per socket:","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"#!/usr/bin/env sh\n#SBATCH -N 2 # two nodes\n#SBATCH -n 4 # four MPI ranks in total\n#SBATCH --ntasks-per-socket 1 # one MPI rank per socket\n#SBATCH -o sl_mpi_multinode_%j.out\n#SBATCH -A pc2-mitarbeiter\n#SBATCH -p all\n#SBATCH -t 00:02:00\n#=\nml lang JuliaHPC # load Julia module (system specific!)\nsrun -n 4 julia --project -t 1 $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\nexit\n# =#\nusing MPI\nusing ThreadPinning\n\nMPI.Init()\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nMPI.Barrier()\nsleep(2*rank)\nprintln(\"Rank $rank:\")\nprintln(\"\\tHost: \", gethostname())\nprintln(\"\\tCPUs: \", getcpuids())\nprint_affinity_masks()","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"Output (manually cleaned up a bit):","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"Rank 0:\n    Host: n2fpga19\n    CPUs: [0]\n1:   |1000000000000000000000000000000000000000000000000000000000000000|0000000000000000000000000000000000000000000000000000000000000000|\n\nRank 1:\n    Host: n2fpga19\n    CPUs: [64]\n1:   |0000000000000000000000000000000000000000000000000000000000000000|1000000000000000000000000000000000000000000000000000000000000000|\n\nRank 2:\n    Host: n2fpga33\n    CPUs: [0]\n1:   |1000000000000000000000000000000000000000000000000000000000000000|0000000000000000000000000000000000000000000000000000000000000000|\n\nRank 3:\n    Host: n2fpga33\n    CPUs: [64]\n1:   |0000000000000000000000000000000000000000000000000000000000000000|1000000000000000000000000000000000000000000000000000000000000000|","category":"page"},{"location":"examples/ex_mpi/#Hybrid:-MPI-Threads","page":"MPI and MPI + Threads","title":"Hybrid: MPI + Threads","text":"","category":"section"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"If your MPI-parallel application is multithreaded (i.e. multiple Julia threads per MPI rank), you can use pinthreads(:affinitymask) to pin Julia threads of each MPI rank according to the affinity mask set by SLURM (according to the user-specified options). If you don't use pinthreads(:affinitymask), the Julia threads are only bound to a range of CPU-threads, they can migrate, and they can also overlap (occupy the same CPU-thread). See Process Affinity Mask for more information.","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"Multinode example, 1 MPI rank per socket, 25 threads per rank:","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"#!/usr/bin/env sh\n#SBATCH -N 2\n#SBATCH -n 4\n#SBATCH --ntasks-per-socket 1\n#SBATCH --cpus-per-task 25\n#SBATCH -o sl_hybrid_multinode_affinitymask_%j.out\n#SBATCH -A pc2-mitarbeiter\n#SBATCH -p all\n#SBATCH -t 00:02:00\n#=\nml lang JuliaHPC\nsrun -n 4 julia --project -t 25 $(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')\nexit\n# =#\nusing MPI\nusing ThreadPinning\npinthreads(:affinitymask)\n\nMPI.Init()\nnranks = MPI.Comm_size(MPI.COMM_WORLD)\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nMPI.Barrier()\nsleep(2*rank)\nprintln(\"Rank $rank:\")\nprintln(\"\\tHost: \", gethostname())\nprintln(\"\\tCPUs: \", getcpuids())","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"Output:","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"Rank 0:\n    Host: n2cn0853\n    CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\nRank 1:\n    Host: n2cn0853\n    CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]\nRank 2:\n    Host: n2cn0854\n    CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\nRank 3:\n    Host: n2cn0854\n    CPUs: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88]","category":"page"},{"location":"examples/ex_mpi/#Manual","page":"MPI and MPI + Threads","title":"Manual","text":"","category":"section"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"In this section, we describe how you can pin the Julia threads of your MPI ranks manually, that is without any \"help\" from an external affinity mask (e.g. as set by SLURM, see above).","category":"page"},{"location":"examples/ex_mpi/","page":"MPI and MPI + Threads","title":"MPI and MPI + Threads","text":"TODO: pinthreads_mpi","category":"page"},{"location":"refs/likwidpin/#Likwid-Pin","page":"Likwid-Pin","title":"Likwid-Pin","text":"","category":"section"},{"location":"refs/likwidpin/#Index","page":"Likwid-Pin","title":"Index","text":"","category":"section"},{"location":"refs/likwidpin/","page":"Likwid-Pin","title":"Likwid-Pin","text":"Pages   = [\"likwidpin.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/likwidpin/#References","page":"Likwid-Pin","title":"References","text":"","category":"section"},{"location":"refs/likwidpin/","page":"Likwid-Pin","title":"Likwid-Pin","text":"Modules = [ThreadPinning]\nPages   = [\"likwid-pin.jl\"]","category":"page"},{"location":"refs/likwidpin/#ThreadPinning.likwidpin_domains-Tuple{}","page":"Likwid-Pin","title":"ThreadPinning.likwidpin_domains","text":"likwidpin_domains(; onebased)\n\n\nThe likwid-pin compatible domains that are available for the system.\n\n\n\n\n\n","category":"method"},{"location":"refs/likwidpin/#ThreadPinning.likwidpin_to_cpuids-Tuple{AbstractString}","page":"Likwid-Pin","title":"ThreadPinning.likwidpin_to_cpuids","text":"likwidpin_to_cpuids(lpstr; onebased)\n\n\nConvert the given likwid-pin compatible string into a CPU ID list. See pinthreads_likwidpin for more information.\n\n\n\n\n\n","category":"method"},{"location":"refs/likwidpin/#ThreadPinning.pinthreads_likwidpin-Tuple{AbstractString}","page":"Likwid-Pin","title":"ThreadPinning.pinthreads_likwidpin","text":"pinthreads_likwidpin(str; onebased)\n\n\nPins Julia threads to CPU-threads based on the given likwid-pin compatible string. Checkout the LIKWID documentation for more information.\n\nIf the keyword argument onebased is set to true, logical indices as well as domain indices start at one instead of zero (likwid-pin default). Note, though, that this doesn't affect the explicit pinning mode where \"physical\" CPU IDs always start at zero.\n\nExamples\n\npinthreads_likwidpin(\"S0:0-3\")\npinthreads_likwidpin(\"M1:0,2,4\")\npinthreads_likwidpin(\"S:scatter\")\npinthreads_likwidpin(\"E:N:4:1:2\")\n\n\n\n\n\n","category":"method"},{"location":"examples/ex_blas/#ex_blas","page":"Autochecking BLAS Thread Settings","title":"Autochecking BLAS Thread Settings","text":"","category":"section"},{"location":"examples/ex_blas/","page":"Autochecking BLAS Thread Settings","title":"Autochecking BLAS Thread Settings","text":"If one runs a multithreaded Julia code that, on each thread, performs linear algebra operations (BLAS/LAPACK calls) one can easily run into performance issues due to an oversubscription of cores by Julia and BLAS threads (see here for a more thorough discussion). Fortunately, ThreadPinning.jl provides some (basic) autochecking functionality that highlights potential problems and suggests improvements.","category":"page"},{"location":"examples/ex_blas/","page":"Autochecking BLAS Thread Settings","title":"Autochecking BLAS Thread Settings","text":"Concretely, you can provide the keyword argument blas=true to threadinfo. This will show some of your BLAS settings and will color-indicate whether they are likely to be ok (green) or suboptimal (red). If you also provide hints=true, ThreadPinning.jl will try to provide concrete notes and warnings that (hopefully) help you to tune your settings.","category":"page"},{"location":"examples/ex_blas/#OpenBLAS","page":"Autochecking BLAS Thread Settings","title":"OpenBLAS","text":"","category":"section"},{"location":"examples/ex_blas/","page":"Autochecking BLAS Thread Settings","title":"Autochecking BLAS Thread Settings","text":"(Image: openblas)","category":"page"},{"location":"examples/ex_blas/#Intel-MKL","page":"Autochecking BLAS Thread Settings","title":"Intel MKL","text":"","category":"section"},{"location":"examples/ex_blas/","page":"Autochecking BLAS Thread Settings","title":"Autochecking BLAS Thread Settings","text":"(Image: mkl)","category":"page"},{"location":"explanations/blas/#BLAS","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"","category":"section"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"This page is concerned with the performance and pinning issues that can occur if you run a multithreaded Julia code that, on each thread, performs linear algebra operations (BLAS/LAPACK calls). In this case, one must ensure that cores aren't oversubscribe due to the two levels of multithreading.","category":"page"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"Relevant discourse threads, see here and here.","category":"page"},{"location":"explanations/blas/#OpenBLAS","page":"Julia Threads + BLAS Threads","title":"OpenBLAS","text":"","category":"section"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"If OPENBLAS_NUM_THREADS=1, OpenBLAS uses the calling Julia thread(s) to run BLAS computations, i.e. it \"reuses\" the Julia thread that runs a computation.\nIf OPENBLAS_NUM_THREADS=N>1, OpenBLAS creates and manages its own pool of BLAS threads (N in total). There is one BLAS thread pool (for all Julia threads).\nJulia default: OPENBLAS_NUM_THREADS=8 (Julia version ≤ 1.8) and OPENBLAS_NUM_THREADS=Sys.CPU_THREADS (Julia version ≥ 1.8).","category":"page"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"When you start Julia in multithreaded mode, i.e. julia -tX or JULIA_NUM_THREADS=X, it is generally recommended to set OPENBLAS_NUM_THREADS=1 or, equivalently, BLAS.set_num_threads(1). Given the behavior above, increasing the number of BLAS threads to N>1 can very easily lead to worse performance, in particular when N<<X! Hence, if you want to or need to deviate from unity, make sure to \"jump\" from OPENBLAS_NUM_THREADS=1 to OPENBLAS_NUM_THREADS=# of cores or similar.","category":"page"},{"location":"explanations/blas/#Intel-MKL","page":"Julia Threads + BLAS Threads","title":"Intel MKL","text":"","category":"section"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"Given MKL_NUM_THREADS=N, MKL starts N BLAS threads per Julia thread that makes a BLAS call.\nDefault: MKL_NUM_THREADS=# of physical cores, i.e. excluding hyperthreads. (Verified experimentally but would be good to find a source for this.)","category":"page"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"When you start Julia in multithreaded mode, i.e. julia -tX or JULIA_NUM_THREADS=X, we recommend to set MKL_NUM_THREADS=(# of cores)/X or, equivalently, BLAS.set_num_threads((# of cores)/X) (after using MKL). Unfortunately, the default is generally suboptimal as soon as you don't run Julia with a single thread. Hence, make sure to tune the settings appropriately.","category":"page"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"Side comment: It is particularly bad / confusing that OpenBLAS and MKL behave very differently for multithreaded Julia.","category":"page"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"warning: Warning\nBe aware that calling an MKL function (for the first time) can spoil the pinning of Julia threads! A concrete example is discussed here. TLDR: You want to make sure that MKL_DYNAMIC=false. Apart from setting the environment variable you can also dynamically call ThreadPinning.mkl_set_dynamic(0). Note that, by default, ThreadPinning.jl will warn you if you call one of the pinning functions while MKL_DYNAMIC=true.","category":"page"},{"location":"explanations/blas/#threadinfo(;-blastrue,-hintstrue)","page":"Julia Threads + BLAS Threads","title":"threadinfo(; blas=true, hints=true)","text":"","category":"section"},{"location":"explanations/blas/","page":"Julia Threads + BLAS Threads","title":"Julia Threads + BLAS Threads","text":"To automatically detect whether you (potentially) have suboptimal BLAS thread settings, you can provide the keyword arguments blas=true and hints=true to threadinfo. An example can be found here.","category":"page"},{"location":"#ThreadPinning.jl","page":"ThreadPinning","title":"ThreadPinning.jl","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"ThreadPinning.jl allows you to pin Julia threads to specific CPU-threads (i.e. \"hardware threads\" or, equivalently, \"CPU processors\") via functions, environment variables, or Julia preferences. Especially for applications running on HPC clusters, this is often absolutely crucial to achieve optimal performance and/or obtain reliable benchmarks (see Why pin Julia threads?).","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"note: Note\nBe aware that Julia implements task-based multithreading: M user tasks get scheduled onto N Julia threads. While this package allows you to pin Julia threads to CPU-threads,  it is generally not safe to assume that a computation (started with Threads.@spawn or Threads.@threads) will run on or even stay on a certain Julia thread (see this discourse post for more information). If you want this guarantee, you can use our @tspawnat macro instead.","category":"page"},{"location":"#Installation","page":"ThreadPinning","title":"Installation","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"Note: Only Linux is supported!","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"The package is registered. Hence, you can simply use","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"] add ThreadPinning","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"to add the package to your Julia environment.","category":"page"},{"location":"#Prerequisites","page":"ThreadPinning","title":"Prerequisites","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"To gather information about the hardware topology of the system (e.g. sockets and memory domains), ThreadPinning.jl uses lscpu. The latter must therefore be available (i.e. be on PATH), which should automatically be the case on virtually all linux systems.","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"In the unlikely case that lscpu isn't already installed on your system, here are a few ways to get it","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"install util-linux via your system's package manager or manually from here\ndownload the same as a Julia artifact: util_linux_jll.jl","category":"page"},{"location":"#Autoupdate-setting","page":"ThreadPinning","title":"Autoupdate setting","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"By default, ThreadPinning.jl queries the system topology using lscpu on startup (i.e. at runtime). This is quite costly but is unfortunately necessary since you might have precompiled the package on one machine and use it from another (think e.g. login and compute nodes of a HPC cluster). However, you can tell ThreadPinning.jl to permanently skip this autoupdate at runtime and to always use the system topology that was present at compile time (i.e. when precompiling the package). This is perfectly save if you don't use the same Julia depot on different machines, in particular if you're a \"standard user\" that uses Julia on a desktop computer or laptop, and can reduce the package load time significantly. To do so, simply call ThreadPinning.Prefs.set_autoupdate(false).","category":"page"},{"location":"#Terminology-in-This-Package","page":"ThreadPinning","title":"Terminology in This Package","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"CPU: Chip that sits in a socket and (almost always) hosts multiple CPU-cores.\nCPU-cores: Physical processor cores of the CPU.\nCPU-threads: Hardware threads (a.k.a. \"virtual cores\") within the CPU-cores.\nCPU ID: Unique ID that identifies a specific CPU-thread. (This is somewhat inconsistent but has been chosen for brevity and backwards-compatibility reasons.)","category":"page"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"If the system supports SMT (\"hyperthreading\"), there are more CPU-threads than CPU-cores (most commonly a factor of two more). Independent of the CPU vendor, we refer to all but the first CPU-threads in a core as hyperthreads (order is taken from lscpu). The latter are highlighted differently in output, see threadinfo().","category":"page"},{"location":"#Noteworthy-Alternatives","page":"ThreadPinning","title":"Noteworthy Alternatives","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"Simply setting JULIA_EXCLUSIVE=1 will pin Julia threads to CPU-threads in \"physical order\" (i.e. as specified by lscpu), which might or might not include hyperthreads.\npinthreads or likwid-pin (CLI tool) from LIKWID.jl\nThis discourse thread discusses issues with alternatives like numactl","category":"page"},{"location":"#Acknowledgements","page":"ThreadPinning","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"ThreadPinning","title":"ThreadPinning","text":"CI infrastructure is provided by the Paderborn Center for Parallel Computing (PC²)","category":"page"},{"location":"examples/ex_core2core_latency/#Core-to-Core-Latency","page":"Measuring Core-to-Core Latency","title":"Core-to-Core Latency","text":"","category":"section"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"Let's measure the inter-core latencies of one of the compute nodes of Noctua 1 at PC2.","category":"page"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"using ThreadPinning\nlatencies = ThreadPinning.bench_core2core_latency()","category":"page"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"40×40 Matrix{Float64}:\n   0.0   217.05  204.85  206.0   203.3   204.95  211.7   205.2   209.5   210.1   209.65  209.3   198.7   194.95  …  271.1   267.5   265.0   260.85  266.9   267.15  265.8   266.7   265.55  258.85  262.1   263.95  269.4\n 215.55    0.0   214.65  215.15  219.8   222.1   219.55  217.1   223.1   224.7   220.25  219.45  213.2   214.6      266.6   269.45  269.85  270.6   271.25  271.1   267.45  265.65  263.15  259.85  260.85  263.2   267.45\n 224.2   214.75    0.0   216.05  217.35  219.25  216.45  212.95  219.15  224.4   221.45  219.65  214.55  215.75     270.35  272.55  270.95  275.0   272.15  272.95  267.5   264.15  260.35  263.2   260.8   262.2   264.65\n 218.4   216.7   211.9     0.0   220.05  218.5   213.2   215.35  225.85  226.7   220.15  218.7   218.6   216.1      266.85  265.75  266.0   265.8   264.7   265.25  259.7   260.9   260.25  258.6   259.6   262.75  262.0\n 221.95  218.5   217.25  212.7     0.0   221.6   220.15  223.75  226.15  224.0   219.45  220.2   214.35  219.3      264.55  267.0   262.6   262.35  264.2   262.2   262.2   263.25  262.4   262.35  264.3   263.7   262.55\n 219.85  212.5   214.6   216.25  218.75    0.0   221.5   221.45  222.6   223.8   227.35  222.8   217.95  221.55  …  265.75  267.95  263.8   264.5   265.4   262.95  265.7   264.55  261.9   263.7   265.25  259.95  261.35\n 219.15  214.0   214.65  217.8   218.85  217.9     0.0   217.15  227.75  225.6   224.05  217.4   216.8   215.15     266.85  269.95  267.85  264.1   262.55  266.15  267.6   267.1   266.25  263.75  260.95  264.4   267.9\n   ⋮                                       ⋮                                       ⋮                             ⋱                            ⋮                                       ⋮\n 269.75  265.85  263.4   265.65  265.0   266.8   265.8   264.15  261.35  258.3   262.65  264.45  265.5   268.05     216.55  221.35  219.5   221.05  220.1   211.45    0.0   219.05  219.45  214.95  213.8   212.75  214.9\n 267.95  265.7   262.2   262.75  262.0   266.35  264.1   260.45  257.4   264.05  268.05  259.85  264.6   265.4      218.6   225.85  226.8   219.3   220.75  215.7   215.95    0.0   221.05  218.35  214.5   214.2   217.3\n 265.65  262.7   263.2   261.8   261.7   260.8   260.95  257.55  259.15  262.05  264.95  263.1   259.55  259.75  …  221.15  223.75  222.8   226.45  226.25  221.05  221.8   219.6     0.0   215.2   216.55  220.45  222.2\n 264.4   263.0   265.85  263.6   265.35  257.25  254.2   258.5   261.6   259.95  259.45  262.3   262.65  259.25     219.0   217.95  218.6   223.2   220.75  215.1   215.2   218.25  216.35    0.0   215.75  216.8   220.15\n 258.75  262.2   264.2   262.55  262.4   262.6   259.05  258.65  257.5   259.4   265.45  260.1   260.2   261.6      217.4   220.85  219.15  218.05  214.5   214.15  215.8   224.5   217.2   217.35    0.0   218.75  222.8\n 264.5   263.35  257.0   262.9   258.65  264.95  266.05  260.75  259.15  264.8   263.95  265.5   267.3   265.35     221.95  222.65  224.25  221.05  220.95  216.5   220.25  220.5   217.2   218.5   218.05    0.0   223.1\n 266.6   266.35  262.65  262.2   264.45  267.2   266.8   264.25  263.75  262.75  264.5   266.55  267.4   271.6      223.9   226.0   225.6   228.65  225.3   219.85  218.25  220.55  220.75  217.1   220.2   225.15    0.0","category":"page"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"Of course, it is easier to make sense of the result if we visualize it. Here, we use Plots.jl's heatmap function.","category":"page"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"using Plots\nheatmap(latencies; c = :viridis, frame=:box)","category":"page"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"(Image: core2core.png)","category":"page"},{"location":"examples/ex_core2core_latency/","page":"Measuring Core-to-Core Latency","title":"Measuring Core-to-Core Latency","text":"The two sockets / CPUs of the system with 20 cores each are clearly visible since the inter-core latency of cores on different sockets is, expectedly, higher than the same for cores sitting on the same socket / in the same CPU. Note that due to fluctuations in our imperfect benchmark the result is not precisely symmetric (which, of course, it should be in theory).","category":"page"}]
}
